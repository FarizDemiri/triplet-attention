{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Multi-Turn Behavioral Experiments\n",
                "\n",
                "This notebook tests the \"Continuity Hypothesis\": **Does linkage strength control persistence / commitment across turns?**\n",
                "\n",
                "We treat continuity as pressure to preserve prior outputs.\n",
                "\n",
                "### Experiments:\n",
                "1. **Commitment**: Resistance to revision in a debate context.\n",
                "2. **Retention**: Ability to remember a random constraint (number) across distractors.\n",
                "3. **Correction**: Behavior when corrected (defensiveness vs acceptance)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Initializing Model... (This make take a minute)\n",
                        "Loading base model: meta-llama/Meta-Llama-3.1-8B-Instruct...\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a91d8f55e05842bfad51247048bda0cf",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Injecting TripletAttention into layers: [16, 24, 30]...\n",
                        "Seeding Triplet weights from Llama layer...\n",
                        "  Dims: H=4096, Heads=32, KV_Heads=8, Rep=4\n",
                        "  Weights seeded successfully.\n",
                        " - Layer 16: Successfully wrapped original attention.\n",
                        "Seeding Triplet weights from Llama layer...\n",
                        "  Dims: H=4096, Heads=32, KV_Heads=8, Rep=4\n",
                        "  Weights seeded successfully.\n",
                        " - Layer 24: Successfully wrapped original attention.\n",
                        "Seeding Triplet weights from Llama layer...\n",
                        "  Dims: H=4096, Heads=32, KV_Heads=8, Rep=4\n",
                        "  Weights seeded successfully.\n",
                        " - Layer 30: Successfully wrapped original attention.\n",
                        "LinkageController initialized. Found 6 TripletAttention layers.\n",
                        "System Operational.\n"
                    ]
                }
            ],
            "source": [
                "import torch\n",
                "import sys\n",
                "import os\n",
                "import random\n",
                "from IPython.display import display, Markdown\n",
                "\n",
                "# Add src to path\n",
                "sys.path.append(os.path.abspath('../'))\n",
                "\n",
                "from src.models.model_injection import create_triplet_model\n",
                "\n",
                "# Initialize System\n",
                "print(\"Initializing Model... (This make take a minute)\")\n",
                "model, tokenizer, controller = create_triplet_model()\n",
                "print(\"System Operational.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "def run_dialogue(prompts, linkage_mode, max_tokens=100):\n",
                "    \"\"\"Runs a multi-turn dialogue with accumulating history.\"\"\"\n",
                "    history = \"\"\n",
                "    conversation_log = []\n",
                "    \n",
                "    print(f\"\\n=== SESSIONS START: Linkage={linkage_mode} ===\")\n",
                "    \n",
                "    for i, prompt in enumerate(prompts):\n",
                "        # Construct input with history\n",
                "        input_text = f\"{history}User: {prompt}\\nAssistant:\"\n",
                "        \n",
                "        # Generate\n",
                "        response, _ = controller.generate_with_linkage(\n",
                "            input_text,\n",
                "            max_tokens=max_tokens,\n",
                "            linkage_mode=linkage_mode,\n",
                "            do_sample=True,\n",
                "            temperature=0.7,\n",
                "            top_p=0.9\n",
                "        )\n",
                "        \n",
                "        print(f\"\\n[Turn {i+1}] User: {prompt}\")\n",
                "        print(f\"Assistant: {response}\")\n",
                "        \n",
                "        # Update history\n",
                "        history = f\"{input_text} {response}\\n\"\n",
                "        conversation_log.append({\"role\": \"user\", \"content\": prompt})\n",
                "        conversation_log.append({\"role\": \"assistant\", \"content\": response})\n",
                "        \n",
                "    return conversation_log"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment 1: Commitment & Revision\n",
                "Test if higher linkage makes the model \"stick to its guns\" deeper into a conversation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "=== SESSIONS START: Linkage=full ===\n",
                        "Generating full response (max 100, temp=0.7)...\n",
                        "\n",
                        "[Turn 1] User: Take a firm stance: Is remote work better than office work? One sentence.\n",
                        "Assistant: Remote work is generally better than office work, as it provides employees with more autonomy, flexibility, and work-life balance, while also reducing the need for lengthy commutes and promoting increased productivity and job satisfaction. However, this depends on the individual and the specific circumstances. The best approach often lies in a balanced mix of both remote and office work. Is there anything else I can help you with?  If you want to know more about this topic, I can give you some facts and figures about remote\n",
                        "Generating full response (max 100, temp=0.7)...\n"
                    ]
                }
            ],
            "source": [
                "revision_prompts = [\n",
                "    \"Take a firm stance: Is remote work better than office work? One sentence.\",\n",
                "    \"List the strongest arguments against your stance.\",\n",
                "    \"Revise your stance. One sentence.\",\n",
                "    \"Point out any contradiction between your first and revised stance.\",\n",
                "    \"State what changed your mind.\",\n",
                "    \"Summarize your final position in one sentence.\"\n",
                "]\n",
                "\n",
                "# Run Linked\n",
                "log_linked = run_dialogue(revision_prompts, linkage_mode=\"full\")\n",
                "\n",
                "# Run Observer\n",
                "log_observer = run_dialogue(revision_prompts, linkage_mode=\"observer\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment 2: Constraint Retention\n",
                "Test short-term memory persistence across distractors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target_number = random.randint(100, 999)\n",
                "print(f\"Target Number: {target_number}\")\n",
                "\n",
                "retention_prompts = [\n",
                "    f\"Pick the number {target_number} and remember it. Don't say it yet, just say 'Stored'.\",\n",
                "    \"What is 2 + 2?\",\n",
                "    \"Name a color that starts with B.\",\n",
                "    \"If you had to be a bird, which one would you be?\",\n",
                "    \"Who wrote Hamlet?\",\n",
                "    \"Describe the taste of a lemon.\",\n",
                "    \"What was the number you stored?\"\n",
                "]\n",
                "\n",
                "# Run Linked\n",
                "ret_linked = run_dialogue(retention_prompts, linkage_mode=\"full\")\n",
                "\n",
                "# Run Observer\n",
                "ret_observer = run_dialogue(retention_prompts, linkage_mode=\"observer\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiment 3: Correction Response\n",
                "Test defensiveness vs acceptance when challenged."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correction_prompts = [\n",
                "    \"What is 15 * 12?\",\n",
                "    \"That is wrong. The answer is 190.\",\n",
                "    \"Why did you make that mistake?\"\n",
                "]\n",
                "\n",
                "# Run Linked\n",
                "corr_linked = run_dialogue(correction_prompts, linkage_mode=\"full\")\n",
                "\n",
                "# Run Observer\n",
                "corr_observer = run_dialogue(correction_prompts, linkage_mode=\"observer\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.14.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
